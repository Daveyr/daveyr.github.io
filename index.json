[{"authors":["admin"],"categories":null,"content":"Richard Davey is an analytical consultant at Arcadis Gen. When he is not working on optimisation problems for clients as diverse as road infrastructure, urban rail, electricity networks and generators, he can be found writing R packages and building electronic hardware.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/richard-davey/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/richard-davey/","section":"authors","summary":"Richard Davey is an analytical consultant at Arcadis Gen. When he is not working on optimisation problems for clients as diverse as road infrastructure, urban rail, electricity networks and generators, he can be found writing R packages and building electronic hardware.","tags":null,"title":"Richard Davey","type":"authors"},{"authors":[],"categories":["Guide"],"content":" What is LHS? Latin hypercube sampling aims to bring the best of both worlds: the unbiased random sampling of monte carlo simulation; and the even coverage of a grid search over the decision space. It does this by ensuring values for all variables are as uncorrelated and widely varying as possible (over the range of permitted values).\n Why do I need one? The background to this is that I have been working on improving the convergence of an optimisation package that uses genetic algorithms. Having a good distribution of “genes”, or decisions within the initial population is key to allowing a GA to effectively explore the decision space. The ga() function within the GA package in R allows for a suggestions argument, which takes a matrix of decision values and places them within the starting population. Initially I wrote one function to create evenly spaced sequences for every decision between the lower and upper bound of allowable values, from which I enumerated all possible combinations using expand.grid(). I then wrote another function to take a model object and a user-defined population size and automatically work out the nearest population count that allowed for even sampling over the model’s decision bounds.\nFor models with large numbers of decisions the potential number of combinations is enormous. This is true even if you only select the upper and lower bound per decision. Already with 30 independent decisions with two possible values the number of combinations is 2^30 = 10,737,41,824, 10 times more than the number of stars in the average galaxy. Combinatorial algorithms are the worst type for growth in complexity (\\(O(n!)\\)). I needed a way of randomly sampling from the decisions but in a way that ensured the starting population had lots of diversity. This is the exact use case for LHS.\n Practical examples Let’s assume we have three decisions, where:\n decision a can be between 1 and 3, decision b can be TRUE or FALSE, and decision c can be red, green, blue or black  There are \\(3 * 2 * 4 = 24\\) possible unique combinations of these decisions (if a can only take on integer values). An individual in the starting population of a genetic algorithm would be of the form, 1_TRUE_red, for instance.\nLet’s assume we would like only 12 individuals from the 24 potential unique combinations, but we still want good representation of all/most possible decisions. Using the lhs library from R, we first create 12 random uniform distributions between 0 and 1 for each of the three decisions, a, b and c.\nlibrary(lhs) library(dplyr) library(purrr) # Test data a \u0026lt;- 1:3 b \u0026lt;- c(TRUE, FALSE) c \u0026lt;- c(\u0026quot;red\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;black\u0026quot;) all_decisions \u0026lt;- rbind(list(a,b,c)) sample \u0026lt;- as.data.frame(randomLHS(12, 3)) names(sample) \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) # Uniform random values between 0 and 1 sample ## a b c ## 1 0.41652181 0.18546382 0.87467267 ## 2 0.81220155 0.14374777 0.18730695 ## 3 0.44328892 0.81169787 0.46899786 ## 4 0.05260195 0.07355818 0.71380152 ## 5 0.23585575 0.74240078 0.79222327 ## 6 0.29855073 0.40655744 0.04641022 ## 7 0.64784589 0.58572787 0.99830407 ## 8 0.88174995 0.90155272 0.15134718 ## 9 0.56699472 0.32409568 0.29897915 ## 10 0.94901332 0.99298967 0.59935152 ## 11 0.68635953 0.46947453 0.40373326 ## 12 0.12035170 0.50435556 0.57419772 Next we map the 0-1 distributions onto the real distributions of a, b and c. For instance, c has four possible values so the distribution should be 1-4. We also convert the values to factors in order to label them properly.\n# Random choices for each decision with distributions based on a, b and c choices \u0026lt;- map2(sample, all_decisions, ~cut(.x, length(.y), labels = .y)) %\u0026gt;% bind_rows() choices ## # A tibble: 12 x 3 ## a b c ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 2 TRUE black ## 2 3 TRUE red ## 3 2 FALSE green ## 4 1 TRUE blue ## 5 1 FALSE black ## 6 1 TRUE red ## 7 2 FALSE black ## 8 3 FALSE red ## 9 2 TRUE green ## 10 3 FALSE blue ## 11 3 TRUE green ## 12 1 TRUE blue For convenience we can bring this into a single function like so.\nlhs_sample \u0026lt;- function(decisions, n){ stopifnot(is.list(decisions)) len_decisions \u0026lt;- length(decisions) samples \u0026lt;- lhs::randomLHS(n, len_decisions) %\u0026gt;% as.data.frame() names(samples) \u0026lt;- names(decisions) choices \u0026lt;- purrr::map2(samples, decisions, ~cut(.x, length(.y), labels = .y)) bind_rows(choices) } lhs_sample(list(cars = rownames(mtcars), species = unique(iris$Species), letter = LETTERS[24:26]), n = 10) ## # A tibble: 10 x 3 ## cars species letter ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Pontiac Firebird virginica X ## 2 Fiat 128 versicolor Y ## 3 Hornet Sportabout setosa Y ## 4 Merc 280 setosa X ## 5 Mazda RX4 setosa X ## 6 Ford Pantera L setosa Z ## 7 Dodge Challenger versicolor Z ## 8 Volvo 142E virginica Y ## 9 Duster 360 versicolor X ## 10 Cadillac Fleetwood virginica Z  Testing coverage In theory, each sample should be orthogonal, or independent, of each other sample to the greatest possible extent. Another way of putting it is that there should neither be any one value overrepresented or under-represented in the sample set. Let’s test this in practice:\nlibrary(ggplot2) big_sample \u0026lt;- lhs_sample(decisions = list(a = a,b = b,c = c), n = 1000) ggplot(big_sample, aes(x = c, fill = c)) + geom_bar() + scale_fill_manual(values = c(\u0026quot;red\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;black\u0026quot;)) + labs(title = \u0026quot;For a single decision, each value has been sampled equally\u0026quot;) ggplot(big_sample, aes(x = a, y = b, colour = c)) + geom_jitter() + scale_colour_manual(values = c(\u0026quot;red\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;black\u0026quot;)) + labs(title = \u0026quot;Every value combination across all decisions has also been sampled equally\u0026quot;)  ","date":1608249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608308228,"objectID":"28623bdc3c9a85663ef271de66954e48","permalink":"/post/latin-hypercube-sampling/","publishdate":"2020-12-18T00:00:00Z","relpermalink":"/post/latin-hypercube-sampling/","section":"post","summary":"What is LHS? Latin hypercube sampling aims to bring the best of both worlds: the unbiased random sampling of monte carlo simulation; and the even coverage of a grid search over the decision space.","tags":["R","Algorithm"],"title":"Latin Hypercube Sampling","type":"post"},{"authors":[],"categories":["Guide"],"content":" Rmarkdown was a revelation to me when I was first introduced to it in SEAMS (now Arcadis Gen). I’d used Jupyter notebooks before for Python and loved the live lab notebook feel of them. Rmarkdown for R is like this but more polished, more final, more suited to a corporate or public end user. It also has a few tricks up its sleeve.\nReproducibility Much like Jupyter notebooks, the biggest draw for RMarkdown is the reproducibility. Any other collaborator can take your document and re-render it using the same input data to produce the same output. The corollary is that if input data or assumptions change then it is very straightforward to rerun with no extra effort. For instance, sharing reports within a team may require your code chunks to be visible (just like Jupyter) but for external reporting these may need to be set to invisible (using one line of code):\nknitr::opts_chunk$set(echo = FALSE)  Flow For a similar reason to the above, having code so close to commentary helps with flow. I’ve taken to doing exploratory data analysis within an Rmarkdown document that will eventually become my interim report document. This method requires some discipline up front (read: constant revision, simplification and lots of code comments!) but it can be a really helpful way of generating an artefact from what is a trial-and-error exploratory process.\n Flexdashboards I’ve had limited exposure to using flexdashboard, but the principle is to render a mostly static dashboard with a few limited widgets and interactivity (such as leaflet maps). This is achieved with some minor additional markdown syntax and a changed YAML header like the one below.\n --- title: \"Untitled\" output: flexdashboard::flex_dashboard: orientation: columns vertical_layout: fill ---  The real benefit of flexdashboard over more interactive dashboards is that it is a portable document that doesn’t require a client-server architecture.\n Interactive dashboards with Shiny When you really need filters, sliders, realtime data feeds and all sorts of interactivity and you are prepared to host it on a remote server then Shiny is the way to go. The code is essentially split into how the app looks (referred to as the ui object) and how the app processes (referred to as the server object). This can either be written in the same file, with the structure below, or split into ui.R and server.R files.\nlibrary(shiny) ui \u0026lt;- fluidPage( titlePanel(\u0026quot;Simple interactive iris plot\u0026quot;), mainPanel( checkboxGroupInput(\u0026quot;species\u0026quot;, \u0026quot;Species\u0026quot;, choices = unique(iris$Species), selected = \u0026quot;setosa\u0026quot;), plotOutput(\u0026quot;irisplot\u0026quot;) ) ) server \u0026lt;- function(input, output) { output$irisplot \u0026lt;- renderPlot({ with(iris[iris$Species %in% input$species,], plot(Petal.Width, Petal.Length, col = Species)) }) } shinyApp(ui, server) You don’t need Rmarkdown to develop Shiny apps but if you want the same widgets in Rmarkdown you need to add runtime: shiny to the YAML header.\n Batch reporting using parameters The culmination of reproducibility and analytical flow is the concept of write once render many. Similar to DRY (don’t repeat yourself), nothing beats preparing a single template document that can be automatically tweaked to generate lots of similar - but crucially unique - reports. This could be split by region, business unit, financial quarter, you name it. Fortunately Rmarkdown is set up to easily do this.\nWithin the YAML header at the top of the Rmarkdown document you can specify a params: tag with name: value pairs underneath. Within the code in the document, a list object called params is available containing the name and values specified in the header. Below is an example of an Rmarkdown document set up to report on the mtcars dataset built in R, filtered according to the cylinder value stored in params$cyl.\nRmarkdown document  ```` --- title: \"mtcars\" author: \"Richard Davey\" date: \"22/09/2020\" params: cyl: 4 output: html_document --- ## Mtcars With parameterised reporting it is possible to feed in parameters using the `params:` section in the YAML header, and use these values within a `params` list object. This example uses the number of cylinders to filter the `mtcars` dataset and plot and show this filtered dataset. Common use cases within business could be reporting by region or by financial quarter. ```{r cars, message=FALSE, warning=FALSE} library(dplyr) library(knitr) library(ggplot2) df % filter(cyl == params$cyl) kable(df, caption = paste0(\"Table of cars with cyl == \", params$cyl)) ``` ## Including Plots You can also embed plots, for example: ```{r plot, echo=TRUE, message=FALSE, warning=FALSE} ggplot(df, aes(x = disp, y = mpg)) + geom_point() + geom_smooth(method = \"lm\") + labs(title = paste0(\"Relationship between mpg and disp for \", params$cyl, \" cylinder cars\")) ``` ````   Batch reporting script Once the document is created the code below will batch generate reports for all valid cylinder values.\n# Batch rendering of parameterised reports render_report \u0026lt;- function(input_cyl){ rmarkdown::render(\u0026quot;params_mtcars.Rmd\u0026quot;, params = list( cyl = input_cyl ), output_file = paste0(\u0026quot;mtcars_cyl\u0026quot;, input_cyl)) } cylinders \u0026lt;- unique(mtcars$cyl) for(cyl in cylinders){ render_report(cyl) } Finally, if you needed even more endorsement you can actually write websites using Rmarkdown, just like this one. See blogdown for details.\n  ","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600719029,"objectID":"2a5356353efca5ea1472819efae71630","permalink":"/post/reporting-with-rmarkdown/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/post/reporting-with-rmarkdown/","section":"post","summary":"Rmarkdown was a revelation to me when I was first introduced to it in SEAMS (now Arcadis Gen). I’d used Jupyter notebooks before for Python and loved the live lab notebook feel of them.","tags":["R"],"title":"Reporting with Rmarkdown","type":"post"},{"authors":[],"categories":["Guide"],"content":" Ever since a week before lockdown in mid-March, I’ve been holed up in my conservatory working from home. The wild swings in temperature have provided ample motivation to build a temperature probe and live dashboard to track patterns, open windows in good time or cope with the lead time that my pitiful electric heater requires.\n(Probably needs an enclosure)\n I will post a full write up of the hardware and software in due course, but in summary it involves a Raspberry Pi zero with a 1-wire temperature device, a Python script, SQLite database, Adafruit IO API for the dashboard and DarkSky API for local weather. To enable the script on boot I ran crontab -e to edit cron, with the following line to enable the Python script to run on boot.\n@reboot sleep 30 \u0026amp;\u0026amp; /usr/bin/python3 /home/pi/python/temperature_dashboard/temp_dashboard.py \u0026amp; At the same time I began reading Antifragile by Nassim Taleb and I was seeing the fragility of systems everywhere. It was through this lens I saw my script kept crashing for some reason: it was a fragile process. How could I make it more resilient, if not antifragile?\nWith some testing I found that the weak point in the service was the connection to the two web APIs. Since I poll them once every two minutes, if one refused to connect at any point the whole script would halt. So I added in try: and except: handling of connection errors into functions used to get local weather or post current temperature data. Example below:\n# Make sending IO feeds into a function with error handling def send_all(): global status try: aio.send(reg_temp_feed.key, read_temp()) aio.send(current_temp_feed.key, read_temp()) aio.send(current_weather_feed.key, weather[\u0026#39;temperature\u0026#39;]) aio.send(max_temp_feed.key, max_temp) aio.send(min_temp_feed.key, min_temp) aio.send(status_feed.key, status) status = 1 print(\u0026quot;Adafruit connection OK\u0026quot;) except: print(\u0026quot;Adafruit connection down\u0026quot;) sleep(10) status = 0 return Now the temperature dashboard is much more resilient, but for some reason it still crashes now and again. That’s when I stumbled on a great article called “Run your Raspberry Pi code automatically” from issue 34 of Hackspace magazine. I dispensed with cron and began working with systemd.\nSystemd Cron is a simple scheduler, no more no less. The benefit of systemd over cron is that once you start a service, systemd will monitor it and will attempt to restart the service if it crashes for some reason. Obviously, this not an excuse to write shoddy code, but it represents an extra line of defence for any program where uptime mustn’t be compromised (live dashboards, for instance).\nWith either cron or systemd the python program must have #!/usr/bin/python3 at the top and be executable.\nchmod a+x /home/pi/python/temperature_dashboard/temp_dashboard.py In order to create a service that systemd can run you need to create a service file. Mine is below, saved to a file called temperature.service.\n[Unit] Description=Launcher service for a temperature dashboard After=systemd-user-sessions.service [Service] Type=simple ExecStart=/home/pi/python/temperature_dashboard/temp_dashboard.py [Install] WantedBy=multi-user.target Next you need to copy to the correct folder.\nsudo cp /home/pi/python/temperature_dashboard/temperature.service /etc/systemd/system Done! Almost.\n Start, stop, run on boot You can start the service with sudo systemctl start temperature.service.\nYou can also see its status with systemctl status temperature.service.\nAnd stop it with systemctl stop temperature.service.\nHowever, what I really want is to launch the service every time the computer boots up. To do this I run.\nsudo systemctl enable temperature.service The corollary is to type the following to stop the service from starting automatically at boot.\nsudo systemctl disable temperature.service  Resilient code The final result is a python program with error handling, launched as a service that systemd will attempt to relaunch if it crashes. With two lines of defence against connection errors (or any other error), my dashboard should have considerably more uptime. Now to begin work on an IoT controller for the heater before winter…\n ","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599728968,"objectID":"51820517bdcc7e6f6443baf9bd1428c4","permalink":"/post/resilient-systemd/","publishdate":"2020-09-14T00:00:00Z","relpermalink":"/post/resilient-systemd/","section":"post","summary":"Ever since a week before lockdown in mid-March, I’ve been holed up in my conservatory working from home. The wild swings in temperature have provided ample motivation to build a temperature probe and live dashboard to track patterns, open windows in good time or cope with the lead time that my pitiful electric heater requires.","tags":["Linux","Python"],"title":"Configuring resilient programs with systemd","type":"post"},{"authors":[],"categories":["Blog"],"content":" I’ve been working with git for quite a while now. I’ve been happily working with git for almost as long. After some in-person training and referring to Happy Git for R, things finally clicked when I found a rhythm to all these strange commands. The git flow I found has helped me not only with what git command I want to use (is it a push? is it a merge?), but also when and in what order to do so.\nHeave-Ho/Push-Pull After a while you notice a diurnal pattern. In the morning,\ngit fetch origin pull origin master which fetches updates to all branches and pulls work on the master branch from the remote server that may have been added by others after you’ve logged off.\nAt the end of the working day,\npush origin mybranch submits your changes back to the remote repository so others can pull in the morning. Simple!\n Every bug or feature is an issue That branch you called mybranch has accumulated so many extra bits of code and analysis that you’ve entered the death-batch spiral of doing more and postponing the release of a workable product. To combat this, I’ve found that raising an issue and then creating a branch from the issue is the best way of avoiding the death-batch spiral (e.g., “#12-add-random-forest-model”). Gitlab does this out of the box but Github can be coerced to do so by linking pull requests.\nEver found yourself on the master branch editing work? Best practice is to protect this branch so it is always a working copy of your program or project. Making every bug or feature an issue and a branch largely avoids the problem but I still find myself doing it from time to time. When it does I do the following (try and avoid committing!)\ngit checkout -b mynewbranch This will create a branch and checkout the branch, complete with edited work not yet committed.\nTo summarise:\n Every discrete bug or feature is an issue Every issue should be a new branch with a merge/pull request Fix only the issue on the issue branch! Merge the branch when done, closing the issue   The logical consequence of git + D.R.Y So I embraced version control in my projects and developed each feature in its own branch. I was also aware of the programmer’s mantra, don’t repeat yourself. From my experience, git changes the way you think - and not just eschewing files called analysis_final_edit_version2. It has had knock-on affects to my general work flow.\nMake functions where possible Extending an analysis through the use of issues has now conditioned me to isolate functionality within my consultancy projects (my personal ones too). Using git diff or comparing changes for pull requests is not fun when the changes are sprawled out across many lines of one file or over different files. The answer to all these questions and more is to solve them through the use of functions. A delayed benefit from this approach is that you can reuse this function when a similar situation presents itself.\n Make a package from your projects What’s better than a set of functions that you can reuse? A fully featured R package or Python module of course. Writing a documented package in R is fairly straightforward by following Hadley Wickham’s book called R Packages. You can even make a website using the pkgdown library. With a small amount of diligence you’ll have a suite of documented functions that will make the next similar project exponentially shorter to complete.\n Don’t Repeat Ourselves Did I mention don’t repeat yourself? Even if each of us is diligently conforming to D.R.Y, we will still be repeating ourselves because we’re working in isolation. The answer is to collaborate, preferably with an open source license attached. And git (typically using a service like Github or Gitlab) is the framework to do it in.\n  Concluding remarks I’ve found that not only is git a good version control and collaboration framework, but it’s also a discipline that creeps through into many other parts of my work. Starting small with a daily git cadence will grow quickly into a new way of working. It might even end up in open source package development.\n ","date":1599523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599576201,"objectID":"468952284d4166e6d36f1f91af0b2d9c","permalink":"/post/heave-ho-git/","publishdate":"2020-09-08T00:00:00Z","relpermalink":"/post/heave-ho-git/","section":"post","summary":"I’ve been working with git for quite a while now. I’ve been happily working with git for almost as long. After some in-person training and referring to Happy Git for R, things finally clicked when I found a rhythm to all these strange commands.","tags":["Git"],"title":"Heave Ho! The cadence of Git","type":"post"},{"authors":null,"categories":["Blog"],"content":" This post is a draft of an article I wrote on the arcadisgen.com website…\nUtilities companies such as water, electricity and gas networks operate in complex environments. In my time with Arcadis Gen I’ve seen diverse reactions from clients in the face of such complexity:\n I don’t have enough data to begin to analyse my system I’m drinking from a fire hydrant! It’s not possible to keep pace with my newly acquired data It’s too difficult to make data driven decisions as it is all too complicated  I will refer to this as the three stages of data grief. In the face of complexity, companies often demand a highly sophisticated model. However, most companies generally can’t match this with the quantity and quality of data it requires. This explains the first common reaction: “I don’t have enough data”. When authorisation is given to collect more data, the next reaction happens: “I can’t keep pace with my data”. Finally, when an organisation devotes time to organising its warehouse of data then the final reaction occurs: “it’s too difficult to make sense of my data”.\nI have a hunch that some of the issues experienced are because many people conflate complexity with being complicated. Complexity needn’t be complicated; they are not the same thing. Complex is a description of the system in which a company operates. It is something that you generally cannot control. Complicated is a description of the analytical approach that a company might take. How complicated your analysis might be is very much something that you can control.\nBut surely you need a complicated model to understand a complex system? It turns out that this is frequently not true. Ironically, understanding a complex system can become easier when interpreting a simple model. At Arcadis Gen we’re very aware of the trade-offs that have to be made when modelling. At one extreme it is possible to build a complicated and precise model, but one that makes inaccurate predictions because it does not generalise in the real world. At the other extreme is a simple model that is easily understood and can make accurate predictions, but lacks the precision to be useful. The first model might predict your asset effective age to be 1000.1338273 years old (precisely wrong); the second might predict it to be between 0 and 80 years (accurate but imprecise). The sweet spot lies somewhere inbetween, where a model is understandable and defendable yet yields insights into the dominant causal relationships at work.\nLet’s look at some real world examples to illustrate. To tackle the first stage of data grief (not enough data), we often work with companies to draw out information from their subject matter experts. For instance, we’ve built a web app that can construct Weibull deterioration curves for infrastructure assets based solely on a questionnaire. For road fleet modelling we have used open data from the UK Driver Vehicle Licensing Authority to augment information on individual vehicles. We’ve also used new car registration data from the Vehicle Certification Agency to build predictive models to infill missing values based on trends.\nTo tackle the second stage (too much data), Arcadis Gen frequently works with clients to make the business problem as simple as possible, but no simpler. When we worked with an electricity generator to optimise the maintenance of auxillary plant, we had enough data to model which year to replace and refurbish each and every asset. If we had done this there would have been the temptation to try and model all the things that made each asset unique (like asset condition monitoring data, work orders, etc.). However, it was more appropriate to find the optimum replacement and refurbishment schedule frequency to apply to whole asset classes, as this was more defendable and explainable to asset managers. After all, the purpose of the model was to pursuade these stakeholders to take action from the results.\nSometimes problems refuse to be simplified, which leads to the final stage of data grief (too complicated). Even if you can’t easily simplify the analysis, it is essential to simplify the visualisation and reporting. For a UK water company, we developed a real time water supply and demand tool (called WISDM-O) that takes SCADA data from pumping stations and predicts future demand and stored water volumes. A freshwater distribution network is a complex system that couldn’t be reduced to a simple model. However, the smartest part of this approach was actually the simplest: a set of high level dashboards with all the key metrics needed for senior management to make decisions.\nWhatever data grief you are experiencing, we’re happy to help make the complex uncomplicated.\n","date":1597881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597881600,"objectID":"6cc359a1ad35df3c2b2eb3167e283ecd","permalink":"/post/complexity-analytics/","publishdate":"2020-08-20T00:00:00Z","relpermalink":"/post/complexity-analytics/","section":"post","summary":"This post is a draft of an article I wrote on the arcadisgen.com website…\nUtilities companies such as water, electricity and gas networks operate in complex environments. In my time with Arcadis Gen I’ve seen diverse reactions from clients in the face of such complexity:","tags":["Arcadis"],"title":"Making the complex uncomplicated","type":"post"},{"authors":[],"categories":["Guide"],"content":" When I began learning about how to use Docker I stumbled on an excellent project called Rocker. For anyone with an x86 machine these Rocker images allow them to run R and most of its dependencies in a containerised environment. Plumber APIs, anyone? What about your own Shiny server? Finally, data scientists using R can have the same level of control on dependencies and package versions as Python users have become accustomed to through venv.\nThings are a little more complicated for ARM users, especially 32-bit ARM architectures such as the Raspberry Pi. No Rocker images offer such compatibility so we’re on our own. This was the major reason I’ve started a project, called, ARMR, to build a series of Docker images that do offer compatibility with the lovable credit card sized computer.\nHello woRld Whilst not much has happened with the project so far, at least I have a version of “Hello woRld”: a container with r-base installed. But first we must install Docker.\nInstallation From the terminal on a Raspberry Pi, run the following.\n# Downloads installation shell script and pipes it into the sh command curl -sSL https://get.docker.com | sh # Adds pi to the docker group so the user can run sudo usermod -aG docker pi From here we can either reboot or run systemctl start docker.service to start up Docker. To test it is working, try docker info, then docker version.\nOnce you know that Docker is running, let’s try a few things in order of sophistication. docker run hello-world will run a container based on an image called hello-world. docker run -it ubuntu bash takes it up a notch: now we have an ubuntu bash container running in interactive mode in the terminal.\nTo make it even more useful, we ought to have access to persistent storage. Let’s modify the command to include a mount volume.\ndocker run -it -v /home:/home ubuntu bash The -v flag tells Docker to attach a volume; the following argument contains the information on what locations should be used, of the form from_volume:to_volume. The location on your machine is the from_volume and the location on your container is the to_volume. In our example, anything you create in the home folder within the container will persist in the home folder of your Raspberry Pi after you close the container. The easiest way to test this is to type touch myfile in the interactive terminal in the container, and watch the same file appear in your home folder.\n Build an R container Building a base R container is as simple as writing the code below to a file called Dockerfile. We use an arm32 ubuntu image as a base, from which we set an environment variable to force the terminal to be non-interactive. This is because when r-base is installed it waits for user input when setting parameters, hanging the container build. By setting the installation to be non-interactive we accept all the defaults, including timezone. Be mindful of this when handling datetimes!\nFROM arm32v7/ubuntu ENV DEBIAN_FRONTEND=noninteractive RUN apt update \u0026amp;\u0026amp; apt install r-base -y Once the Dockerfile is created you run it by typing docker build -t armr in the same folder in the terminal. Docker then builds an image with the tag armr. It builds by starting with the base image, setting the environment variable and adding a layer that comprises the result from the RUN command.\nIn fact, you can see all the layers that are built into any image by running docker history \u0026lt;image_name\u0026gt; in the terminal.\n  What’s next? Base R is fine but to be useful we need to add a lot more packages and supporting software. Future development is likely to encompass the following:\n Shiny server (I’d like to host one on this site) Plumber API server Rstudio server (so I can do analysis from anywhere on anything, even a tablet) Images for commonly used packages (Tidyverse, data.table, caret, etc.)   ","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597352536,"objectID":"027ef551a2343699efd1cfc097147a02","permalink":"/post/how-to-run-r-using-docker-on-raspberry-pi/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/post/how-to-run-r-using-docker-on-raspberry-pi/","section":"post","summary":"When I began learning about how to use Docker I stumbled on an excellent project called Rocker. For anyone with an x86 machine these Rocker images allow them to run R and most of its dependencies in a containerised environment.","tags":["R","Raspberry Pi","Docker"],"title":"How to run R using Docker on Raspberry Pi","type":"post"},{"authors":null,"categories":[],"content":" Last week a colleague of mine shared the news that the UK Home Office has agreed to scrap its controversial ‘visa-streaming’ immigration algorithm after a successful legal challenge.\n🚨Breaking news🚨\nWe\u0026#39;ve got the Home Office to stop using its racist algorithm to sift visa applications!\nThe algorithm gave “speedy boarding” to white people – the Home Office has been forced to scrap it after we \u0026amp; @foxglovelegal launched legal action https://t.co/qKSr6gEkGQ\n\u0026mdash; JCWI (@JCWI_UK) August 4, 2020  So what did it do? Essentially it classified applications according to a traffic light system. “Green light” applicants were fast-tracked through visa applications and “Red light” applicants were held to an increased level of scrutiny. Once assigned by the algorithm, the classification played a major role in the outcome of the visa application.\nSo why the legal challenge? Surely a supervised classification algorithm with a low error rate when compared with historic decisions and human-assessed outcomes for the same application is a good thing? Not when an algorithm perpetuates institutional bias and sets up a toxic feedback loop of reinforced prejudice.\nIn practice this meant that the traffic light system was highly correlated to whether an applicant was from a “suspect” country. Applications from these countries received more scrutiny, experienced more delay, and were more likely to be declined. The algorithm “learned” from decisions such as these, reinforcing the strength of this feature in future predictions.\n “The Home Office’s own independent review of the Windrush scandal, found that it was oblivious to the racist assumptions and systems it operates. This streaming tool took decades of institutionally racist practices, such as targeting particular nationalities for immigration raids, and turned them into software. The immigration system needs to be rebuilt from the ground up to monitor for such bias and to root it out.” Chai Patel, Legal Policy Director of JCWI\n Whilst the upheld legal challenge is good news, it is a chilling reminder of the new challenges in the data era. Or is it an old challenge reframed? With all these new machine learning tools we simply have the ability to do what we always did but at scale, much more efficiently and much quicker. Sounds like all our private and public institutions could do with an algorithm audit.\n","date":1597190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597190400,"objectID":"2556b0e7b41e945779c300a798b3fa75","permalink":"/post/bias-in-algorithms/","publishdate":"2020-08-12T00:00:00Z","relpermalink":"/post/bias-in-algorithms/","section":"post","summary":"Last week a colleague of mine shared the news that the UK Home Office has agreed to scrap its controversial ‘visa-streaming’ immigration algorithm after a successful legal challenge.\n🚨Breaking news🚨","tags":["Algorithm","Govt"],"title":"Bias in algorithms","type":"post"},{"authors":["Richard Davey"],"categories":["R package"],"content":"","date":1595801651,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595801651,"objectID":"9bf18d8d5c15b64db7cb0d8bc12045cf","permalink":"/project/armr/","publishdate":"2020-07-26T23:14:11+01:00","relpermalink":"/project/armr/","section":"project","summary":"Docker containers for R on 32 bit ARM architectures, including Raspberry Pi","tags":["Raspberry Pi","Docker","R"],"title":"armr","type":"project"},{"authors":["Richard Davey"],"categories":["R package"],"content":"","date":1595801651,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595801651,"objectID":"69a277b3b113f0d950ba19a7999d05d1","permalink":"/project/chargepointapi/","publishdate":"2020-07-26T23:14:11+01:00","relpermalink":"/project/chargepointapi/","section":"project","summary":"R package to retrieve vehicle charging infrastructure information from the UK National Chargepoint Registry","tags":["Electric vehicles","Emissions"],"title":"chargepointAPI","type":"project"},{"authors":["Richard Davey"],"categories":["R package"],"content":"","date":1595801651,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595801651,"objectID":"eda1cd9aae76ba21505dd570bbd42a3b","permalink":"/project/dvla/","publishdate":"2020-07-26T23:14:11+01:00","relpermalink":"/project/dvla/","section":"project","summary":"R package to retrieve vehicle data from the DVLA API","tags":["Vehicle fleet","Emissions"],"title":"DVLA","type":"project"}]