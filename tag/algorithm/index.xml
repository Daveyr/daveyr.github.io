<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithm | Algorist</title>
    <link>/tag/algorithm/</link>
      <atom:link href="/tag/algorithm/index.xml" rel="self" type="application/rss+xml" />
    <description>Algorithm</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><lastBuildDate>Fri, 18 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Algorithm</title>
      <link>/tag/algorithm/</link>
    </image>
    
    <item>
      <title>Latin Hypercube Sampling</title>
      <link>/post/latin-hypercube-sampling/</link>
      <pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/latin-hypercube-sampling/</guid>
      <description>


&lt;div id=&#34;what-is-lhs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is LHS?&lt;/h2&gt;
&lt;p&gt;Latin hypercube sampling aims to bring the best of both worlds: the unbiased random sampling of monte carlo simulation; and the even coverage of a grid search over the decision space. It does this by ensuring values for all variables are as uncorrelated and widely varying as possible (over the range of permitted values).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-do-i-need-one&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why do I need one?&lt;/h2&gt;
&lt;p&gt;The background to this is that I have been working on improving the convergence of an optimisation package that uses genetic algorithms. Having a good distribution of “genes”, or decisions within the initial population is key to allowing a GA to effectively explore the decision space. The &lt;code&gt;ga()&lt;/code&gt; function within the GA package in R allows for a &lt;em&gt;suggestions&lt;/em&gt; argument, which takes a matrix of decision values and places them within the starting population. Initially I wrote one function to create evenly spaced sequences for every decision between the lower and upper bound of allowable values, from which I enumerated all possible combinations using &lt;code&gt;expand.grid()&lt;/code&gt;. I then wrote another function to take a model object and a user-defined population size and automatically work out the nearest population count that allowed for even sampling over the model’s decision bounds.&lt;/p&gt;
&lt;p&gt;For models with large numbers of decisions the potential number of combinations is enormous. This is true even if you only select the upper and lower bound per decision. Already with 30 independent decisions with two possible values the number of combinations is 2^30 = 10,737,41,824, 10 times more than the number of stars in the average galaxy. Combinatorial algorithms are the worst type for growth in complexity (&lt;span class=&#34;math inline&#34;&gt;\(O(n!)\)&lt;/span&gt;). I needed a way of randomly sampling from the decisions but in a way that ensured the starting population had lots of diversity. This is the exact use case for LHS.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;practical-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Practical examples&lt;/h2&gt;
&lt;p&gt;Let’s assume we have three decisions, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;decision a can be between 1 and 3,&lt;/li&gt;
&lt;li&gt;decision b can be TRUE or FALSE, and&lt;/li&gt;
&lt;li&gt;decision c can be red, green, blue or black&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are &lt;span class=&#34;math inline&#34;&gt;\(3 * 2 * 4 = 24\)&lt;/span&gt; possible unique combinations of these decisions (if a can only take on integer values). An individual in the starting population of a genetic algorithm would be of the form, &lt;code&gt;1_TRUE_red&lt;/code&gt;, for instance.&lt;/p&gt;
&lt;p&gt;Let’s assume we would like only 12 individuals from the 24 potential unique combinations, but we still want good representation of all/most possible decisions. Using the lhs library from R, we first create 12 random uniform distributions between 0 and 1 for each of the three decisions, a, b and c.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lhs)
library(dplyr)
library(purrr)

# Test data
a &amp;lt;- 1:3
b &amp;lt;- c(TRUE, FALSE)
c &amp;lt;- c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;black&amp;quot;)
all_decisions &amp;lt;- rbind(list(a,b,c))
sample &amp;lt;- as.data.frame(randomLHS(12, 3))
names(sample) &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;)

# Uniform random values between 0 and 1
sample&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             a          b          c
## 1  0.41652181 0.18546382 0.87467267
## 2  0.81220155 0.14374777 0.18730695
## 3  0.44328892 0.81169787 0.46899786
## 4  0.05260195 0.07355818 0.71380152
## 5  0.23585575 0.74240078 0.79222327
## 6  0.29855073 0.40655744 0.04641022
## 7  0.64784589 0.58572787 0.99830407
## 8  0.88174995 0.90155272 0.15134718
## 9  0.56699472 0.32409568 0.29897915
## 10 0.94901332 0.99298967 0.59935152
## 11 0.68635953 0.46947453 0.40373326
## 12 0.12035170 0.50435556 0.57419772&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we map the 0-1 distributions onto the real distributions of a, b and c. For instance, c has four possible values so the distribution should be 1-4. We also convert the values to factors in order to label them properly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Random choices for each decision with distributions based on a, b and c
choices &amp;lt;- map2(sample, all_decisions,
                ~cut(.x, length(.y), labels = .y)) %&amp;gt;%
  bind_rows()
  
choices&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 3
##    a     b     c    
##    &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;
##  1 2     TRUE  black
##  2 3     TRUE  red  
##  3 2     FALSE green
##  4 1     TRUE  blue 
##  5 1     FALSE black
##  6 1     TRUE  red  
##  7 2     FALSE black
##  8 3     FALSE red  
##  9 2     TRUE  green
## 10 3     FALSE blue 
## 11 3     TRUE  green
## 12 1     TRUE  blue&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For convenience we can bring this into a single function like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lhs_sample &amp;lt;- function(decisions, n){
  stopifnot(is.list(decisions))
  len_decisions &amp;lt;- length(decisions)
  samples &amp;lt;- lhs::randomLHS(n, len_decisions) %&amp;gt;%
    as.data.frame()
  names(samples) &amp;lt;- names(decisions)
  choices &amp;lt;- purrr::map2(samples, decisions,
                  ~cut(.x, length(.y), labels = .y))
  bind_rows(choices)
}

lhs_sample(list(cars = rownames(mtcars), species = unique(iris$Species), letter = LETTERS[24:26]), n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##    cars               species    letter
##    &amp;lt;fct&amp;gt;              &amp;lt;fct&amp;gt;      &amp;lt;fct&amp;gt; 
##  1 Pontiac Firebird   virginica  X     
##  2 Fiat 128           versicolor Y     
##  3 Hornet Sportabout  setosa     Y     
##  4 Merc 280           setosa     X     
##  5 Mazda RX4          setosa     X     
##  6 Ford Pantera L     setosa     Z     
##  7 Dodge Challenger   versicolor Z     
##  8 Volvo 142E         virginica  Y     
##  9 Duster 360         versicolor X     
## 10 Cadillac Fleetwood virginica  Z&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-coverage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing coverage&lt;/h2&gt;
&lt;p&gt;In theory, each sample should be orthogonal, or independent, of each other sample to the greatest possible extent. Another way of putting it is that there should neither be any one value overrepresented or under-represented in the sample set. Let’s test this in practice:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
big_sample &amp;lt;- lhs_sample(decisions = list(a = a,b = b,c = c), n = 1000)

ggplot(big_sample, aes(x = c, fill = c)) +
  geom_bar() +
  scale_fill_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;black&amp;quot;)) +
  labs(title = &amp;quot;For a single decision, each value has been sampled equally&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-18-latin-hypercube-sampling/index.en_files/figure-html/coverage_chart-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(big_sample, aes(x = a, y = b, colour = c)) +
  geom_jitter() +
  scale_colour_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;black&amp;quot;)) +
  labs(title = &amp;quot;Every value combination across all decisions has also been sampled equally&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-18-latin-hypercube-sampling/index.en_files/figure-html/coverage_chart-2.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fork the algorithm</title>
      <link>/post/fork-the-algorithm/</link>
      <pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/fork-the-algorithm/</guid>
      <description>


&lt;p&gt;Watching scenes like these makes me feel like I’m already living in a dystopian future.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This is amazing. The future is here.&lt;br&gt;&lt;br&gt;‘Fuck the algorithm’&lt;br&gt; &lt;a href=&#34;https://t.co/k4Vny0L4tF&#34;&gt;pic.twitter.com/k4Vny0L4tF&lt;/a&gt;&lt;/p&gt;&amp;mdash; Carole Cadwalladr (@carolecadwalla) &lt;a href=&#34;https://twitter.com/carolecadwalla/status/1295277889412304897?ref_src=twsrc%5Etfw&#34;&gt;August 17, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;The protests are of course in response to the UK Education Secretary Gavin Williamson’s decision to use an algorithm to determine A-level and GCSE grades for students this year.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bias in algorithms</title>
      <link>/post/bias-in-algorithms/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/post/bias-in-algorithms/</guid>
      <description>


&lt;p&gt;Last week a colleague of mine shared the news that the UK Home Office has agreed to scrap its controversial ‘visa-streaming’ immigration algorithm after a successful legal challenge.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;🚨Breaking news🚨&lt;br&gt;&lt;br&gt;We&amp;#39;ve got the Home Office to stop using its racist algorithm to sift visa applications!&lt;br&gt;&lt;br&gt;The algorithm gave “speedy boarding” to white people – the Home Office has been forced to scrap it after we &amp;amp; &lt;a href=&#34;https://twitter.com/Foxglovelegal?ref_src=twsrc%5Etfw&#34;&gt;@foxglovelegal&lt;/a&gt; launched legal action &lt;a href=&#34;https://t.co/qKSr6gEkGQ&#34;&gt;https://t.co/qKSr6gEkGQ&lt;/a&gt;&lt;/p&gt;&amp;mdash; JCWI (@JCWI_UK) &lt;a href=&#34;https://twitter.com/JCWI_UK/status/1290561862807953412?ref_src=twsrc%5Etfw&#34;&gt;August 4, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;So what did it do? Essentially it classified applications according to a traffic light system. “Green light” applicants were fast-tracked through visa applications and “Red light” applicants were held to an increased level of scrutiny. Once assigned by the algorithm, the classification played a major role in the outcome of the visa application.&lt;/p&gt;
&lt;p&gt;So why the legal challenge? Surely a supervised classification algorithm with a low error rate when compared with historic decisions and human-assessed outcomes for the same application is a good thing? Not when an algorithm perpetuates institutional bias and sets up a toxic feedback loop of reinforced prejudice.&lt;/p&gt;
&lt;p&gt;In practice this meant that the traffic light system was highly correlated to whether an applicant was from a “suspect” country. Applications from these countries received more scrutiny, experienced more delay, and were more likely to be declined. The algorithm “learned” from decisions such as these, reinforcing the strength of this feature in future predictions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The Home Office’s own independent review of the Windrush scandal, found that it was oblivious to the racist assumptions and systems it operates. This streaming tool took decades of institutionally racist practices, such as targeting particular nationalities for immigration raids, and turned them into software. The immigration system needs to be rebuilt from the ground up to monitor for such bias and to root it out.” Chai Patel, Legal Policy Director of JCWI&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Whilst the upheld legal challenge is good news, it is a chilling reminder of the new challenges in the data era. Or is it an old challenge reframed? With all these new machine learning tools we simply have the ability to do what we always did but at scale, much more efficiently and much quicker. Sounds like all our private and public institutions could do with an algorithm audit.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
